{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modal SNA MAP473D, Ecole Polytechnique, 2018-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\mc}[1]{\\mathcal{#1}}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\Q}{\\mathbb{Q}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\DeclareMathOperator{\\Var}{Var}$\n",
    "$\\newcommand{\\Pth}{\\P_\\theta}$\n",
    "$\\newcommand{\\Lth}{L_\\theta}$\n",
    "$\\newcommand{\\Eth}{\\E_\\theta}$\n",
    "$\\newcommand{\\Vth}{\\Var_\\theta}$\n",
    "$\\newcommand{\\one}{\\mathbb{1}}$\n",
    "$\\DeclareMathOperator{\\d}{d\\!}$\n",
    "$\\newcommand{\\pscal}[2]{\\langle{#1},{#2}\\rangle}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Soumission du TP pour évaluation:** \n",
    "- Remplir ce notebook et déposer sur le moodle le fichier notebook \".ipynb\" ainsi qu'une sauvegarde (export) au format \".html\".\n",
    "- Les réponses aux questions théoriques peuvent être saisies (en latex) dans le notebook ou bien être rendues sur feuille libre avant 17h45 aux enseignants. \n",
    "- Les dépots sur le moodle doivent se faire le vendredi 29 mars avant 20h. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noms du binôme:** Duy Anh Alexandre / Joao Rocha Oliviera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 - Echantillonnage d’importance, changements de probabilités gaussiens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1. Rappel de cours : changements de probabilités, échantillonnage préférentiel et grandes déviations\n",
    "Soit $X$ une v.a. à valeurs dans $\\R^d$, définie sur un espace de probabilité $(\\Omega, \\mc{F}, \\P)$. On notera $\\E$ l'espérance associée à la probabilité $\\P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Principe général du changements de probabilité\n",
    "Soit $L:\\R^d \\to \\R^*_+$ une fonction mesurable telle que $$\\E[L(X)]=1.$$\n",
    "\n",
    "Notons $\\Q$ la loi de probabilité sur $(\\Omega, \\mc{F})$, de densité $L(X)$ par rapport à $\\P$, et $\\E_\\Q$ l'espérance associée. Par définition de $\\Q$, pour toute fonction $g: \\R^d \\to \\R$ mesurable bornée, on a\n",
    "<a id='eq:Principe:IS'></a>\n",
    "$$ \\tag{1}\n",
    "    \\E[g(X)]=\\E \\left[ \\frac{g(X)}{L(X)} \\, L(X) \\right]=\\E_{\\mathbb{Q}}\\left[ \\frac{g(X)}{L(X)} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Changements de probabilité inspirés par les grandes déviations (cas $d=1$)<a id='sec:12'></a>\n",
    "On définit l'ensemble convexe\n",
    "$$ \n",
    "    \\mc{D} = \\{ \\theta \\in \\R~:  \\E\\!\\left[e^{\\theta X} \\right]<\\infty \\}\n",
    "$$  \n",
    "et la fonction $\\psi: \\mc{D} \\to \\R$ dite _fonction génératrice des cumulants_ de $X$, par \n",
    "$$\n",
    "    \\psi(\\theta) = \\log \\E\\!\\left[ e^{\\theta X} \\right].\n",
    "$$\n",
    "On fait l'hypothèse que la loi de $X$ sous $\\P$ est telle que $\\mc{D}$ est non vide. Pour tout $\\theta \\in \\mc{D}$, on peut définir la loi $\\Pth$, de densité $\\Lth(X)$ par rapport à $\\P$ où\n",
    "$$\n",
    "    \\Lth(x) = \\exp\\left(\\theta x - \\psi(\\theta)\\right), \\qquad x \\in \\R,\n",
    "$$\n",
    "(noter que $\\E[\\Lth(X)] =1$); de sorte que la famille de lois $\\{\\Pth, \\theta \\in \\mc{D} \\}$ donne des exemples de changements de probabilité $\\P \\to \\Pth$. Lorsque $\\theta = 0$, $\\P$ et $\\Pth$ coïncident puisque $L_0(x) =1$. Dans la suite, on utilisera les notations $\\Eth$ et $\\Vth$ plutôt que $\\E_{\\Pth}$ et $\\Var_{\\Pth}$ respectivement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut montrer que pour tout $\\theta$ dans l'intérieur de $\\mc{D}$, l'espérance et la variance de $X$ sous $\\Pth$ sont liées aux dérivées successives de $\\psi$ (voir par exemple le Corollaire 7.2  dans __[Information and Exponential Families](https://onlinelibrary.wiley.com/doi/book/10.1002/9781118857281)__) \n",
    "<a id='eq_esp_var2'></a>\n",
    "$$\\tag{2}\n",
    "    \\Eth[X]=\\psi'(\\theta),\\qquad \\qquad \n",
    "    \\Vth(X)=\\psi''(\\theta);\n",
    "$$\n",
    "et en particulier, lorsque $\\theta = 0$ est dans l'intérieur de $\\mc{D}$,\n",
    "$$ \n",
    "    \\E[X]=\\psi'(0),\\qquad \\qquad \n",
    "    \\Var(X)=\\psi''(0).\n",
    "$$\n",
    "\n",
    "Le succès de cette technique de changement de loi dépend de la capacité à identifier la loi de $X$ sous $\\Pth$ et à faire des tirages _i.i.d._ sous cette loi.  Par exemple, nous montrerons en section 3.1 que si $X\\sim \\mc{N}(0,1)$ sous $\\P$, alors $X\\sim \\mc{N}(\\theta,1)$ sous $\\Pth$. La transformation de Esscher donnera des formules du même type, lorsque sous $\\P$, $X$ est un processus de Poisson composé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Application à l'approximation Monte Carlo de la fonction de survie\n",
    "Supposons que l'objectif soit d'approcher la quantité $\\P[X>a]$ par une méthode de Monte Carlo. La méthode de Monte Carlo naïve consiste en l'approximation\n",
    "<a id='eq:MC:naif'></a>\n",
    "$$ \\tag{3}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n \\one_{X_k>a} \\quad \n",
    "    \\text{avec $\\{X_k, k \\geq 1 \\}$, indépendantes de même loi que $X$ sous $\\P$.}\n",
    "$$\n",
    "\n",
    "Néanmoins, pour la loi $\\Q$ de densité $L(X)$ par rapport à $\\P$, on a aussi la relation (relire l'équation [(1)](#eq:Principe:IS) avec la fonction $g(x) = \\one_{]a, +\\infty[}(x)$)\n",
    "$$\n",
    "    \\P(X>a) = \\E_\\Q\\left[ L(X)^{-1} \\, \\one_{X>a} \\right].\n",
    "$$ \n",
    "On déduit de cette égalité une autre approximation\n",
    "<a id='eq:MC:loiQ'></a>\n",
    "$$ \\tag{4}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n \\frac{1}{L(Y_k)}\\one_{Y_k>a} \\quad \n",
    "    \\text{avec $\\{Y_k, k \\geq 1 \\}$, indépendantes de même loi que $X$ sous $\\Q$.}\n",
    "$$\n",
    "\n",
    "\n",
    "Dans le cas particulier où $\\Q$ est de la forme $\\Pth$, l'approximation $(4)$ devient\n",
    "<a id='eq:MC:Ptheta'></a>\n",
    "$$ \\tag{5}\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n \\exp\\left( \\psi(\\theta) - \\theta \\, Y_k \\right) \\, \\one_{Y_k>a}, \\qquad\n",
    "    \\text{$\\{Y_k, k \\geq 1 \\}$ indépendantes de même loi que $X$ sous $\\Pth$;}\n",
    "$$\n",
    "la difficulté pour l'utilisateur est de choisir la valeur $\\theta \\in \\mc{D}$ la plus \"efficace\".  La relation [(2)](#eq_esp_var2) suggère de choisir $\\theta$ égal à la solution de $\\psi'(\\theta) = a$ (valeur que nous noterons $\\theta_a$), puisqu'avec ce choix, sous $\\P_{\\theta_a}$, la loi de $X$ est centrée en $a$. Ce n'est pas nécessairement le choix optimal - selon un critère d'optimalité basé sur la réduction de variance (voir section 3.1) - mais cela donne un échantillonneur Monte Carlo plus efficace que l'échantillonneur Monte Carlo naïf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 2. Echantillonnage d'importance pour des v.a. de Bernoulli\n",
    "Les v.a. sont définies sur $(\\Omega, \\mathcal{A}, \\P)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "Soient $X_1, \\cdots, X_n$ des v.a. indépendantes de loi de Bernoulli $B(p_1), \\ldots, B(p_n)$; et $Z_1, \\ldots, Z_n$ des v.a. indépendantes de lois de Bernoulli $B(q_1), \\ldots, B(q_n)$.\n",
    "Montrer que pour toute fonction $g:\\{0,1\\}^n \\to\\mathbb R$ mesurable bornée, on a\n",
    "\\begin{eqnarray*} \n",
    "  \\E \\left[ g(X_1, \\ldots, X_n) \\right] &=& \\E \\left[ g(Z_1, \\ldots, Z_n) \\, \\prod_{i=1}^n \\left(\\one_{Z_i=0}\\frac{1-p_i}{1-q_i}+1_{Z_i=1}\\frac{p_i}{q_i}\\right) \\right]\n",
    "  \\\\\n",
    "                                        &=& \\left( \\prod_{i=1}^n\\frac{1-p_i}{1-q_i} \\right) \\  \\E\\left[ g(Z_1, \\ldots,Z_n) \\ \\prod_{i=1}^n\\left(\\frac{p_i(1-q_i)}{q_i(1-p_i)}\\right)^{Z_i} \\right]\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Réponse:_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 2:\n",
    "Soit $n\\ge 1$, $p,q\\in ]0,1[$; et $S,S'$ deux v.a. respectivement de loi binomiale $\\operatorname{Binom}(n,p)$ et de loi binomiale $\\operatorname{Binom}(n,q)$. Déduire de la question précédente que pour toute fonction $f:\\{0, \\ldots, n\\} \\to \\R$, on a\n",
    "$$\n",
    "    \\E \\left[ f(S) \\right] \n",
    "    = \\E \\left[ f(S')\\left(\\frac{p}{q}\\right)^{S'}\\left(\\frac{1-p}{1-q}\\right)^{n-S'} \\right] \n",
    "    = \\left(\\frac{1-p}{1-q}\\right)^n \\E\\left[f(S')\\left(\\frac{p(1-q)}{q(1-p)}\\right)^{S'} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Réponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 3:\n",
    "Pour $S$ de loi binomiale $\\operatorname{Binom}(n,p)$ et $x \\in ]0,p[$, estimer $\\P(S \\le nx)$ des trois façons suivantes:\n",
    "- en utilisant la fonction de calcul exact de cette probabilité `scipy.stats.binom.cdf`,\n",
    "- par un Monte Carlo standard en simulant $N$ copies de $S$ avec la fonction `np.random.binomial` ou `scipy.stats.binom.rvs`,\n",
    "- en utilisant la question 2 et en simulant $N$ copies de $S'$.\n",
    "\n",
    "On pourra choisir par exemple $n=300$, $p=0.25$, $x=0.001$, $q=x$ et $N=10^6$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loi theorique: 3.2989749474660756e-38 \n",
      " Monte Carlo: 0.0 \n",
      " Methode question 2:  3.297006216313699e-38\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as sps\n",
    "import numpy as np\n",
    "n = 300\n",
    "p = .25\n",
    "x = .001\n",
    "q = x\n",
    "N = int(1e6)\n",
    "C = ((1-p)/(1-q)) ** n\n",
    "\n",
    "#Loi theorique\n",
    "B = sps.binom.cdf( x*n, n, p)\n",
    "\n",
    "#Monte Carlo\n",
    "X_p = np.random.binomial(n, p, N)\n",
    "m = np.mean(X_p <= n * x)\n",
    "\n",
    "#Methode 2\n",
    "X = np.random.binomial(n, q , N)\n",
    "K = ((p * (1-q))/(q * (1-p))) ** X\n",
    "t = C * np.mean((X <= n * x) * K)\n",
    "\n",
    "print(\" Loi theorique:\", B,\"\\n Monte Carlo:\", m,'\\n Methode question 2: ', t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 3. Changement de probabilité dans un modèle gaussien\n",
    "Dans cette section, $X \\sim \\mathcal{N}(0,1)$ sous $\\P$.  Nous cherchons à calculer $\\P(X>a)$ puis $\\P(|X|>a)$ pour des valeurs assez élevées de $a$ de sorte que l'événement puisse être qualifié de rare (bien entendu, ces deux probabilités sont liées par la relation $\\P(|X|>a)=2\\P(X>a)$). Nous avons la relation, pour tout $a>0$,\n",
    "$$\n",
    "    \\frac{1}{a+1/a} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}} \n",
    "    \\le \\P(X>a) \\le \n",
    "    \\frac{1}{a} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}},\n",
    "$$\n",
    "ce qui entraine quand $a \\to +\\infty$,\n",
    "$$ \n",
    "    \\P(X>a) \\simeq \\frac{1}{a }\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{a^2}{2}}.\n",
    "$$\n",
    "Ainsi, comme $e^{\\frac{2.15^2}{2}} \\simeq 10$, $\\P(X>a)$ et$\\P(|X|>a)$ sont de l'ordre de $10^{-\\left(\\frac{a}{2.15}\\right)^2}$: des valeurs de $a$ comprises dans l'intervalle $[5,8]$ correspondent à notre champ d'investigation. Nous considérons deux changements de probabilité pour améliorer le comportement de l'échantillonneur de Monte Carlo naïf\n",
    "$$\n",
    "    \\P(X>a) \\simeq \\frac{1}{n} \\sum_{k=1}^n \\one_{X_k > a} \\qquad \\text{où $\\{X_k, k \\geq 1 \\}$ i.i.d. $\\mathcal{N}(0,1)$.}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Changement de probabilité par décentrage\n",
    "Ce changement de probabilité repose sur le changement de loi $\\P \\to \\Pth$ décrit en [section 1.2](#sec:12).  \n",
    "\n",
    "La fonction génératrice des cumulants de $X$ est définie sur $\\mc{D} = \\R$ et vaut $\\psi(\\theta) =\\theta^2/2$. Identifions la loi $\\Pth$: la relation [(1)](#eq:Principe:IS) appliquée avec $\\Q = \\Pth$, montre que pour toute fonction $f:\\R \\to \\R_+$ mesurable\n",
    "\\begin{align*}\n",
    "  \\Eth \\left[ f(X) \\right] = \\E \\left[ f(X) L_\\theta(X) \\right] \n",
    "  &= \\frac{1}{\\sqrt{2\\pi}} \\int_\\R f(x) \\exp \\left(\\theta x - \\theta^2/2 \\right) \\exp(-x^2/2) \\d x  \\\\\n",
    "  &= \\frac{1}{\\sqrt{2\\pi}} \\int_\\R f(x) \\exp \\left(-(x-\\theta)^2/2 \\right) \\d x\n",
    "\\end{align*}\n",
    "on en déduit que sous $\\Pth$, $X \\sim \\mathcal{N}(\\theta,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 4.\n",
    "Simuler $n \\gg 1$ gaussiennes indépendantes de loi $\\mc{N}(0,1)$ avec lesquelles on estimera $\\P(X>a)$ d'abord par l'estimateur naïf [(3)](#eq:MC:naif), puis par l'estimateur [(5)](#eq:MC:Ptheta) appliqué avec $\\theta = \\theta_a$, la solution de l'équation $\\psi'(\\theta) = a$.  Pour chacun des estimateurs, utiliser le TCL pour construire un intervalle de confiance asymptotique de probabilité de couverture $95 \\%$. Comparer à la valeur exacte de $\\P(X>a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "Intervalle de confiance, méthode 1 : [ 0.0 0.0 ]\n",
      "2.880958081875289e-07\n",
      "Intervalle de confiance, méthode 1 : [ 2.838381101016792e-07 2.9235350627337857e-07 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n= 100000\n",
    "a=5\n",
    "X=np.random.randn(n)\n",
    "X_emp=np.mean(X>a)\n",
    "print(X_emp)\n",
    "X_std=np.std(X>a)\n",
    "erreur=1.96*X_std/np.sqrt(n)\n",
    "print(\"Intervalle de confiance, méthode 1 : [\",X_emp-erreur,X_emp+erreur,\"]\")\n",
    "\n",
    "theta=a\n",
    "X_theta=X+theta\n",
    "X_theta=np.exp(theta**2/2-theta*X_theta)*(X_theta>a)\n",
    "X_theta_emp=np.mean(X_theta)\n",
    "print(X_theta_emp)\n",
    "X_theta_std=np.std(X_theta)\n",
    "erreur_theta=1.96*X_theta_std/np.sqrt(n)\n",
    "print(\"Intervalle de confiance, méthode 1 : [\",X_theta_emp-erreur_theta,X_theta_emp+erreur_theta,\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 5.\n",
    "La longueur de l'intervalle de confiance est une fonction croissante de la variance de l'estimateur: on veut donc choisir $\\theta$ qui minimise cette variance.  Nous allons montrer que le choix de $\\theta$ fait dans la question précédente pourrait être amélioré si le critère d'optimalité souhaité est de minimiser la variance du nouvel estimateur.\n",
    "\n",
    "a) Montrer que choisir $\\theta$ qui minimise la variance de l'estimateur donné par [(5)](#eq:MC:Ptheta), revient à choisir $\\theta$ qui minimise\n",
    "$$\n",
    "    \\theta \\mapsto \\E\\left[ \\one_{X > a} \\exp\\left(\\frac{\\theta^2}{2} - \\theta X \\right) \\right].\n",
    "$$\n",
    "\n",
    "b) Montrer que cette fonction est convexe et qu'elle possède un unique minimum donné par la solution de\n",
    "$$\n",
    "    \\E\\left[ \\one_{X > a} (\\theta - X) \\exp\\left(- \\theta X \\right) \\right] = 0,\n",
    "$$\n",
    "(on admettra que le modèle est régulier i.e. que l'on permuter dérivée et intégration).\n",
    "\n",
    "c) En déduire que prendre $\\theta = \\theta_a$ est meilleur que prendre $\\theta = 0$; et que la valeur de $\\theta$ la plus efficace est en fait supérieure à $\\theta_a$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _Réponse:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Changement de probabilité par modification de la variance\n",
    "Le changement de probabilité par modification de la variance repose sur l'observation suivante: pour toute fonction $f: \\R \\to \\R$ mesurable bornée, on a\n",
    "$$\n",
    "    \\E[f(X)] = \\E \\left[\\sigma f(\\sigma X)e^{-\\frac{(\\sigma^2-1)}{2} X^2} \\right] \n",
    "    = \\E_{\\P_\\sigma}\\left[ \\sigma f(X) \\, e^{-\\frac{(\\sigma^2-1)}{2 \\sigma^2}X^2} \\right]\n",
    "$$\n",
    "où sous $\\P_\\sigma$, $X \\sim \\mc{N}(0, \\sigma^2)$. On peut donc estimer $\\E[f(X)]$ en appliquant la méthode de Monte Carlo naïve, ou en utilisant l'estimateur\n",
    "\\begin{equation} \\label{eq:MC:Psi} \n",
    "    \\E[f(X)] \\simeq \\frac{\\sigma}{n} \\sum_{k=1}^n \\, f(Y_k) \\, e^{-\\frac{(\\sigma^2-1)}{2 \\sigma^2} Y_k^2} , \\qquad\n",
    "  \\text{$\\{Y_k, k \\geq 1 \\}$ i.i.d.  de loi $\\mc{N}(0,\\sigma^2)$.}\n",
    "\\end{equation}\n",
    "Là encore, le choix de $\\sigma$ n'est pas anodin sur la qualité de l'estimateur, et selon un critère de réduction de variance, on choisira $\\sigma = \\sigma_\\star$ où $\\sigma_\\star$ est solution de\n",
    "$$\n",
    "    \\operatorname{argmin}_{\\sigma > 0} \\sigma^2 \\E \\left[ f^2(\\sigma X) e^{-(\\sigma^2-1)X^2} \\right].\n",
    "$$\n",
    "Bien souvent, on n'a pas d'expression explicite de cette valeur optimale, et la détermination de $\\sigma_\\star$ est une des principales difficultés de l'implémentation de cette méthode. Dans l'application numérique suivante, nous utiliserons une technique rudimentaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 6.\n",
    "Se donner $\\mc{S}$, une version discrétisée de l'intervalle $\\left[\\frac{1}{2} a, 3a \\right]$.  Simuler $n_1$ gaussiennes centrées réduites, et avec ce même jeu de simulations, pour chaque valeur $\\sigma \\in \\mc{S}$, calculer une approximation Monte Carlo de la quantité \n",
    "$$\n",
    "    \\sigma^2 \\E \\left[ \\one_{|\\sigma X| >a} e^{-(\\sigma^2-1)X^2} \\right].\n",
    "$$\n",
    "En déduire une approximation $\\hat{\\sigma}_\\star$ de $\\sigma_\\star$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8158158158158155\n"
     ]
    }
   ],
   "source": [
    "a=5\n",
    "Y=np.linspace(0.5*a,3*a,1000)\n",
    "n1=10000\n",
    "opt=0\n",
    "X=np.random.randn(n1)\n",
    "Z=(np.abs(Y[0]*X)>a)*np.exp(-(Y[0]**2-1)*X*X)\n",
    "emp=(Y[0]**2)*np.mean(Z)\n",
    "res=emp\n",
    "for i in Y:\n",
    "    X=np.random.randn(n1)\n",
    "    Z=(np.abs(i*X)>a)*np.exp(-(i**2-1)*X*X)\n",
    "    emp=(i**2)*np.mean(Z)\n",
    "    if emp<res:\n",
    "        res=emp\n",
    "        opt=i\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 7.\n",
    "Simuler $n_2$ autres gaussiennes centrées réduites $X_1, \\dots, X_{n_2}$ ($n_2 \\gg n_1$). Avec ce même jeu de simulations, proposer une estimation de $\\P(|X| >a)$ via la méthode de Monte Carlo naïve, et une estimation via la formule\n",
    "$$\n",
    "    \\P(|X|>a) \\simeq \\frac{\\sigma}{n_2} \\sum_{k=1}^{n_2} \\one_{|\\sigma X_k| > a} e^{-\\frac{(\\sigma^2-1)}{2} X_k^2}\n",
    "$$\n",
    "tout d'abord pour $\\sigma = \\hat{\\sigma}_\\star$ puis pour $\\sigma = a$.\n",
    "\n",
    "Pour chacun des trois cas, utiliser le TCL pour construire un intervalle de confiance asymptotique de probabilité de couverture $95\\%$. Comparer à la valeur exacte de $\\P(|X|>a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Intervalle de confiance naive [ 0.0 0.0 ]\n",
      "moyenne avec sigma étoile 5.700745754420243e-07\n",
      "Intervalle de confiance avec sigma étoile [ 5.642228227916033e-07 5.759263280924452e-07 ] 5.85175265042093e-07\n",
      "moyenne avec a 5.754292201847664e-07\n",
      "Intervalle de confiance avec a [ 5.69598988184819e-07 5.812594521847138e-07 ] 5.830231999947327e-07\n",
      "valeur exacte:  5.733031437583867e-07\n"
     ]
    }
   ],
   "source": [
    "a=5\n",
    "n2=1000000\n",
    "X1=np.random.randn(n2)\n",
    "X1=(X1>a)\n",
    "emp1=np.mean(X1)\n",
    "std1=np.std(X1)\n",
    "erreur1=1.96*std1/np.sqrt(n2)\n",
    "print(emp1)\n",
    "print(\"Intervalle de confiance naive [\",emp1-erreur,emp1+erreur,\"]\")\n",
    "\n",
    "X2=np.random.randn(n2)\n",
    "X2=opt*(np.abs(opt*X2)>a)*np.exp(-(opt**2-1)/2*X2*X2)                        \n",
    "emp2=np.mean(X2)\n",
    "std2=np.std(X2)\n",
    "erreur2=1.96*std2/np.sqrt(n2)\n",
    "print(\"moyenne avec sigma étoile\", emp2)\n",
    "print(\"Intervalle de confiance avec sigma étoile [\",emp2-erreur2,emp2+erreur2,\"]\",100*erreur2)\n",
    "\n",
    "X3=np.random.randn(n2)\n",
    "X3=a*(np.abs(a*X3)>a)*np.exp(-(a**2-1)/2*X3*X3)                        \n",
    "emp3=np.mean(X3)\n",
    "std3=np.std(X3)\n",
    "erreur3=1.96*std3/np.sqrt(n2)\n",
    "print(\"moyenne avec a\", emp3)\n",
    "print(\"Intervalle de confiance avec a [\",emp3-erreur3,emp3+erreur3,\"]\",100*erreur3)\n",
    "print(\"valeur exacte: \", sps.norm.cdf(-a)+sps.norm.sf(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 4. Algorithme adaptatif de moyenne optimale dans le cas gaussien multidimensionnel\n",
    "\n",
    "Dans cette section, sous $\\P$, $X$ est un vecteur gaussien de $\\R^d$ standard, $X \\sim \\mc{N}(0,I_d)$ où $I_d$ désigne la matrice identité de taille $d \\times d$. On note $\\pscal{a}{b}$ le produit scalaire usuel sur $\\R^d$ et $\\|\\cdot \\|$ la norme euclidienne associée. Pour un vecteur $a$, $a'$ est la transposée. Par convention, les vecteurs sont des vecteurs colonnes.\n",
    "\n",
    "On cherche à calculer $\\E[f(X)]$ pour une fonction $f: \\R^d \\to \\R$ mesurable telle que pour tout $\\theta\\in \\R^d$,\n",
    "$$\n",
    "    0<\\E \\left[f^2(X) \\ e^{-\\pscal{\\theta}{X}}\\right]<\\infty.\n",
    "$$\n",
    "Pour ce faire, on peut implémenter un estimateur de Monte Carlo naïf ou exploiter la relation\n",
    "<a id='eq:IS:multidim'></a>\n",
    "$$ \\tag{6}\n",
    "  \\E[h(X)]=\\E \\left[h(X + \\theta) \\, e^{-\\pscal{\\theta}{X}-\\|\\theta\\|^2/2} \\right],\n",
    "$$ </a>\n",
    "valable pour tout $\\theta \\in \\R^d$, et toute fonction $h: \\R^d \\to \\R$ mesurable positive. L'objectif de cette section est d'apprendre, par une procédure adaptative, la valeur optimale du paramètre $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithme de Lelong et Jourdain\n",
    "D'après l'article [Robust adaptive importance sampling for normal random vectors](https://projecteuclid.org/euclid.aoap/1255699541). On déduit de [(6)](#eq:IS:multidim) que pour tout $\\theta \\in \\R^d$,\n",
    "- (i) la v.a.\n",
    "<a id='eq:IS:JL'></a>\n",
    "$$ \\tag{7}\n",
    "    M_n(\\theta) = \\frac{1}{n} \\sum_{k=1}^n f(X_k + \\theta) \\ e^{-\\pscal{\\theta}{X_k}-\\|\\theta\\|^2/2}, \n",
    "    \\qquad \\text{$\\{X_k, k \\geq 0\\}$ v.a. i.i.d. de loi $\\mc{N}(0,1)$}\n",
    "$$\n",
    "est un estimateur sans biais de l'objectif $\\E\\left[f(X)\\right]$,\n",
    "- (ii) sa variance est égale à $n^{-1} \\left( \\mc{V}(\\theta)- \\left(\\E[f(X)] \\right)^2 \\right)$, où\n",
    "$$\n",
    "    \\mc{V}(\\theta) = \\E \\left[f^2(X) e^{- \\pscal{\\theta}{X}+\\|\\theta\\|^2/2} \\right].\n",
    "$$\n",
    "\n",
    "On souhaite donc appliquer l'estimateur [(7)](#eq:IS:JL) avec $\\theta=\\theta_\\star$ où $\\theta_\\star$ minimise $\\mc{V}(\\theta)$. Cependant, en général, lorsque $\\E[f(X)]$ est inconnu, il en est de même pour $\\mc{V}(\\theta)$ et donc $\\theta_\\star$ est défini comme un minimiseur d'une fonction incalculable. Or, sous des conditions d'intégrabilité sur $f$, la fonction $\\mc{V}$ est de classe $C^2$, strictement convexe et possédant un unique minimum $\\theta_\\star$; son gradient $G$ et son Hessien $H$ sont donnés par\n",
    "\\begin{align*}\n",
    "  G(\\theta) &= \\E \\left[ f^2(X) \\left( \\theta -X \\right) e^{- \\pscal{\\theta}{X}+\\|\\theta\\|^2/2} \\right]  \\in \\R^{d \\times 1}, \\\\\n",
    "  H(\\theta) &= \\E \\left[ f^2(X) \\left( I_d + (\\theta-X) (\\theta-X)' \\right) e^{- \\pscal{\\theta}{X}+\\|\\theta\\|^2/2} \\right]  \\in \\R^{d \\times d},\n",
    "\\end{align*}\n",
    "$I_d$ désigne la matrice identité de taille $d \\times d$. \n",
    "\n",
    "Afin d'approcher $\\theta_\\star$, on va utiliser une version bruitée de la _méthode de Newton_: pour $n$ fixé, $\\theta_\\star$ sera approché par $\\theta_n$ défini comme la limite lorsque $k$ tend vers l'infini, de la suite $\\{t_k, k\\ge 0\\}$ satisfaisant à\n",
    "<a id='eq:Newton'></a>\n",
    "$$ \\tag{8}\n",
    "    t_{k+1} = t_k - \\left(H_n(t_k)\\right)^{-1} \\, G_n(t_k)\n",
    "$$\n",
    "où $H_n(t), G_n(t)$ sont les approximations Monte Carlo de $H(t)$ et $G(t)$ calculées à partir des mêmes $n$ réalisations indépendantes $X_1, \\cdots, X_n$ de v.a. $\\mc{N}_d(0,I_d)$:\n",
    "\\begin{align*}\n",
    "    G_n(\\theta) &= \\frac{1}{n}\\sum_{k=1}^n f^2(X_k) (\\theta-X_k)e^{-\\pscal{\\theta}{X_k}+\\|\\theta\\|^2/2}, \\\\\n",
    "    H_n(\\theta) &= \\frac{1}{n}\\sum_{k=1}^n f^2(X_k) \\left(I_d + (\\theta-X_k)(\\theta-X_k)'\\right) e^{-\\pscal{\\theta}{X_k}+\\|\\theta\\|^2/2}.\n",
    "\\end{align*}\n",
    "En pratique, on se fixe un seuil $\\varepsilon > 0$ et on itère la relation [(8)](#eq:Newton) tant que $\\|G_n(t_k)\\|>\\varepsilon$. On obtient alors $\\theta_n$; on prendra pour estimateur de $\\E[f(X)]$ la quantité $M_n(\\theta_n)$ calculée à partir des mêmes tirages $X_1, \\cdots, X_n$ que ceux utilisés pour le calcul de $G_n$ et $H_n$.\n",
    "\n",
    "Les résultats suivants, qui peuvent être démontrés sous hypothèses d'intégrabilité sur la fonction $f$, assurent que la méthode fonctionne (voir l'article [Robust adaptive importance sampling for normal random vectors](https://projecteuclid.org/euclid.aoap/1255699541)): \n",
    "- (i) $M_n(\\theta_n)$ tend presque sûrement vers $\\E[f(X)]$ lorsque $n \\to \\infty$, \n",
    "- (ii) si $\\Var(f(X))>0$, la suite de v.a.\n",
    "$$\n",
    "    \\sqrt{\\frac{n}{\\mc{V}(\\theta_n)-M_n(\\theta_n)^2}}(M_n(\\theta_n)-\\E[f(X)])\n",
    "$$\n",
    "converge en loi vers une loi gaussienne standard,\n",
    "- (iii) $\\sqrt{n}(\\theta_n-\\theta_\\star)$ est asymptotiquement gaussienne centrée (sa variance peut être exprimée, de façon relativement complexe, en fonction des paramètres). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 8.\n",
    "On se limitera dans cette question au cas $d=1$.  On s'intéresse au cas particulier $f:x \\mapsto \\left(e^{x} - K\\right)^+$, qui correspond au calcul du prix d'une option call en finance (pour $y\\in{\\mathbb R}$, on note $y^+:=\\max(y,0)$). \n",
    "Il s'agit d'un exemple jouet puisque $\\E[f(X)]$ a une expression explicite donnée par \n",
    "<a id='eq:BS'></a>\n",
    "$$ \\tag{9}\n",
    "    \\E[f(X)] = e^{\\frac{1}{2}} \\, \\Phi(1-\\ln(K))-K \\, \\Phi(-\\ln(K)), \\qquad \\text{avec} \\quad \\Phi(x):=\\P(X\\le x).\n",
    "$$\n",
    "(cette formule n'est plus valable en dimension $d$ supérieure à $1$).\n",
    "\n",
    "Fixer $n_\\mathrm{max} = 10^4$ et simuler $X_1, \\cdots, X_{n_\\mathrm{max}}$ v.a.  i.i.d. $\\mc{N}(0,1)$.\n",
    "\n",
    "**8.a)** Pour différentes valeurs de $n \\leq n_\\mathrm{max}$, représenter la fonction\n",
    "$$\n",
    "    \\mc{V}_n: \\theta \\mapsto \\frac{1}{n} \\sum_{k=1}^n f^2(X_k) \\exp\\left( - \\pscal{\\theta}{X_k} + \\|\\theta\\|^2/2 \\right).\n",
    "$$\n",
    "  \n",
    "  Tracer aussi la suite $\\{t_k, 1\\leq k \\leq \\min\\{\\tau_\\varepsilon, 100\\}\\}$, où $\\tau_\\varepsilon$ est le premier $k$ tel que $\\|G_n(t_k)\\|\\le \\varepsilon$.  Comment cette suite converge-t-elle ?\n",
    "  \n",
    "On pourra prendre $K=1$, se limiter à tracer $\\mc{V}_n$ sur l'intervalle $[-1,4]$ et prendre $\\varepsilon = 0.05$.  <!--\n",
    "Noter que lorsque $d=1$,\n",
    "$$\n",
    "    \\theta-\\frac{G_n(\\theta)}{H_n(\\theta)} = \n",
    "    \\frac{\\sum_{k=1}^n f^2(X_k) \\left(\\theta^3+(1-2\\theta^2)X_k+\\theta X_k^2 \\right) e^{-\\theta X_k}}\n",
    "    {\\sum_{k=1}^n f^2(X_k) \\left( 1+(\\theta-X_k)^2 \\right) e^{-\\theta X_k}}.\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "nmax=int(1e4)\n",
    "K=1\n",
    "epsilon=0.05\n",
    "X=np.random.randn(nmax)\n",
    "\n",
    "def f(x):\n",
    "    return np.max(np.exp(x)-K,0)\n",
    "\n",
    "n=int(nmax/2)\n",
    "val=np.empty(0)\n",
    "Z=np.linspace(-1,4,1000)\n",
    "for i in Z:\n",
    "    Y=f(f(X))*np.exp(-i*X+i**2/2)\n",
    "    val=np.append(val,np.mean(Y[:n]))\n",
    "\n",
    "plt.plot(Z,val)\n",
    "\n",
    "def g(x,theta):\n",
    "    return f(f(x))*(x-theta)*np.exp(-theta*x+theta**2/2)\n",
    "\n",
    "def h(x,theta):\n",
    "    return f(f(x))*(1+(x-theta)**2)*np.exp(-theta*x+theta**2/2)\n",
    "\n",
    "t=np.empty(100)\n",
    "t[0]=1\n",
    "i=0\n",
    "while i<=100 and np.abs(np.mean(g(X[:n],t[i])))>epsilon:\n",
    "       t[i+1]=t[i]-1/(np.mean(h(X[:n],t[i])))*np.mean(g(X[:n],t[i]))\n",
    "       \n",
    "plt.plot(np.linspace(1,100,1),t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.b)** Tracer sur un même graphique les suites $\\{M_n(0), n \\geq 0\\}$ et $\\{M_n(\\theta_n), n \\geq 0\\}$ pour $n$ variant de $100$ à $n_{\\mathrm{max}}$ par sauts de $50$ ou de $100$ (de façon à accélérer l'algorithme). \n",
    "\n",
    "Pour comparaison, tracer aussi la droite horizontale $y=\\E[f(X)]$ où cette constante est calculée avec la formule [(9)](#eq:BS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.c)** Pour différentes valeurs de $n$, simuler un grand nombre de réalisations de $M_n(0)$ et de $M_n(\\theta_n)$, estimer les variances de ces deux v.a., et tracer l'histogramme de ces réalisations, ainsi que la droite verticale $x=\\E[f(X)]$. Que nous disent ces histogrammes sur l'efficacité des deux méthodes ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Question 9.\n",
    "\n",
    "Reprendre la question précédente en dimension $d$ supérieure, pour la fonction \n",
    "$$\n",
    "    f: x =(x_1,\\dots,x_d)\\mapsto \\left(\\frac{1}{d}\\sum_{i=1}^d e^{x_i}-K\\right)^+,\n",
    "$$ \n",
    "qui correspond au calcul du prix d'un call sur moyenne.  On affichera le tracé des suites $\\{M_n(\\theta_n), n \\geq 0\\}$ et $\\{M_n(0), n \\geq 0\\}$ ainsi que les histogrammes.\n",
    "\n",
    "Vous pourrez utiliser les fonctions d'algèbre linéaire `numpy.dot`, `numpy.inner`, `np.eye`, `np.outer`, aisi que `numpy.linalg.inv` et `numpy.linalg.norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
